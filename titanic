#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed May 23 00:04:46 2018

@author: sweety
"""
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"

import random
import time

import numpy as np

import pandas as pd
pd.options.mode.chained_assignment = None

from sklearn.preprocessing import LabelEncoder
from sklearn import model_selection
from sklearn import metrics
from sklearn import feature_selection


import matplotlib as mpl
import matplotlib.pyplot as plt

import plotly.plotly as py
import plotly.graph_objs as go
import plotly.tools as tls

import graphviz

import seaborn as sns

import itertools

#Common Model Algorithms
from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process
from xgboost import XGBClassifier
from sklearn.exceptions import DataConversionWarning

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings(action='ignore', category=DataConversionWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)

mpl.style.use('ggplot')
sns.set_style('white')

#Fills in the missing values
def completeMissingValues(dataClean):
    for dataSet in dataClean:
        #complete missing age with mean
        dataSet['Age'].fillna(dataSet['Age'].median(), inplace = True)
        #complete embarked with mode
        dataSet['Embarked'].fillna(dataSet['Embarked'].mode()[0], inplace = True)
        #complete Fare with mean
        dataSet['Fare'].fillna(dataSet['Fare'].median(), inplace = True)

    #delete the cabin feature/column and others previously stated to exclude in train dataset
    drop_column = ['PassengerId','Cabin', 'Ticket']
    data_raw.drop(drop_column, axis=1, inplace = True)

    return dataClean

#create new features
def createNewFeatures(dataClean):
    for dataSet in dataClean:
        #creating new feature of family size
        dataSet['FamilySize'] = dataSet['SibSp'] + dataSet['Parch'] + 1
        #create new feature to check if the person is travelling alone
        dataSet['IsAlone'] = 1
        dataSet['IsAlone'].loc[dataSet['FamilySize'] > 1] = 0
        #create new feature for the title
        dataSet['Title'] = dataSet['Name'].str.split(", ", expand=True)[1].str.split(".", expand=True)[0]
        #Create Fare Bins based on Percentiles
        dataSet['FareBin'] = pd.qcut(dataSet['Fare'], 4)
        #Age Bins(using cut or value bins)
        dataSet['AgeBin'] = pd.cut(dataSet['Age'],5)
        #cleanup rare title names
        #print(dataSet['Title'].value_counts())
        stat_min = 10
        #this will create a true false series with title name as index
        title_names = (dataSet['Title'].value_counts() < stat_min)
        #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code
        dataSet['Title'] = dataSet['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)
        #print(dataSet['Title'].value_counts())

    return dataClean

def categorizeData(dataClean):
    label = LabelEncoder()
    for dataset in dataClean:
        dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])
        dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])
        dataset['Title_Code'] = label.fit_transform(dataset['Title'])
        dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])
        dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])

    return dataClean

def cleanData(dataClean):
    dataClean = completeMissingValues(dataClean)
    dataClean = createNewFeatures(dataClean)
    dataClean = categorizeData(dataClean)

    return dataClean

#correlation heatmap of dataset
def correlation_heatmap(df):
    a , ax = plt.subplots(figsize =(14, 12))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)

    sns.heatmap(
        df.corr(),
        cmap = colormap,
        square=True,
        cbar_kws={'shrink':.9 },
        ax=ax,
        annot=True,
        linewidths=0.1,vmax=1.0, linecolor='white',
        annot_kws={'fontsize':12 }
    )

    plt.title('Pearson Correlation of Features', y=1.05, size=15)


def analysis():
    #Discrete Variable Correlation by Survival using
    #group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html
    for x in data_raw_x:
        if data_raw[x].dtype != 'float64' :
            print('Survival Correlation by:', x)
            print(data_raw[[x, Target[0]]].groupby(x, as_index=False).mean())
            print('-'*10, '\n')


    #using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html
    print(pd.crosstab(data_raw['Title'],data_raw[Target[0]]))

    #graph distribution of quantitative data
    fareTrace = go.Box(y=data_raw['Fare'],boxmean=True, name ='Fare Box Plot')
    py.plot([fareTrace])

    ageTrace = go.Box(y = data_raw['Age'], boxmean = True,name ='Age Box Plot')
    py.plot([ageTrace])

    familySizeTrace = go.Box(y = data_raw['FamilySize'], boxmean = True, name ='Family Size Box Plot')
    py.plot([familySizeTrace])

    #Survial wrt Fare
    SurvivedDataForFare = go.Histogram(
            x= data_raw[data_raw['Survived']==1]['Fare'],
            name='Survived')
    DeadDataForFare =go.Histogram(
            x= data_raw[data_raw['Survived']==0]['Fare'],
            name ='Dead')
    SurvivalFareData = [SurvivedDataForFare,DeadDataForFare]
    SurvivalFareLayout = go.Layout(
            barmode='stack',
            title='Survival Fare Hist Plot',
            xaxis=dict(title ='Fare ($)'),
            yaxis = dict(title ='# of Passengers')

            )
    fig1 = go.Figure(data =SurvivalFareData, layout =SurvivalFareLayout)
    py.plot(figure_or_data=fig1)

    #Survial wrt Age
    SurvivedDataForAge = go.Histogram(
            x= data_raw[data_raw['Survived']==1]['Age'],
            name='Survived')
    DeadDataForAge =go.Histogram(
            x= data_raw[data_raw['Survived']==0]['Age'],
            name ='Dead')
    SurvivalAgeData = [SurvivedDataForAge,DeadDataForAge]
    SurvivalAgeLayout = go.Layout(
            barmode='stack',
            title='Survival Age Hist Plot',
            xaxis=dict(title ='Age'),
            yaxis = dict(title ='# of Passengers')

            )
    fig2 = go.Figure(data =SurvivalAgeData, layout =SurvivalAgeLayout)
    py.plot(figure_or_data=fig2)

    #Survial wrt FamilySize
    SurvivedDataForFamilySize = go.Histogram(
            x= data_raw[data_raw['Survived']==1]['FamilySize'],
            name='Survived')
    DeadDataForFamilySize =go.Histogram(
            x= data_raw[data_raw['Survived']==0]['FamilySize'],
            name ='Dead')
    SurvivalFamilySizeData = [SurvivedDataForFamilySize,DeadDataForFamilySize]
    SurvivalFamilySizeLayout = go.Layout(
            barmode='stack',
            title='Survival FamilySize Hist Plot',
            xaxis=dict(title ='FamilySize'),
            yaxis = dict(title ='# of Passengers')

            )
    fig3 = go.Figure(data =SurvivalFamilySizeData, layout =SurvivalFamilySizeLayout)
    py.plot(figure_or_data=fig3)

    #we will use seaborn graphics for multi-variable comparison:
    #graph individual features by survival
    fig, saxis = plt.subplots(2, 3,figsize=(16,12))

    sns.barplot(x = 'Embarked', y = 'Survived', data=data_raw, ax = saxis[0,0])
    sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data_raw, ax = saxis[0,1])
    sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data_raw, ax = saxis[0,2])

    sns.pointplot(x = 'FareBin', y = 'Survived',  data=data_raw, ax = saxis[1,0])
    sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data_raw, ax = saxis[1,1])
    sns.pointplot(x = 'FamilySize', y = 'Survived', data=data_raw, ax = saxis[1,2])

    #graph distribution of qualitative data: Pclass
    #we know class mattered in survival, now let's compare class and a 2nd feature
    fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))

    sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data_raw, ax = axis1)
    axis1.set_title('Pclass vs Fare Survival Comparison')

    sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data_raw, split = True, ax = axis2)
    axis2.set_title('Pclass vs Age Survival Comparison')

    sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data_raw, ax = axis3)
    axis3.set_title('Pclass vs Family Size Survival Comparison')

    #graph distribution of qualitative data: Sex
    #we know sex mattered in survival, now let's compare sex and a 2nd feature
    fig, qaxis = plt.subplots(1,3,figsize=(14,12))

    sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data_raw, ax = qaxis[0])
    axis1.set_title('Sex vs Embarked Survival Comparison')

    sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data_raw, ax  = qaxis[1])
    axis1.set_title('Sex vs Pclass Survival Comparison')

    sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data_raw, ax  = qaxis[2])
    axis1.set_title('Sex vs IsAlone Survival Comparison')

    #more side-by-side comparisons
    fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))

    #how does family size factor with sex & survival compare
    sns.pointplot(x="FamilySize", y="Survived", hue="Sex", data=data_raw,
                  palette={"male": "blue", "female": "pink"},
                  markers=["*", "o"], linestyles=["-", "--"], ax = maxis1)

    #how does class factor with sex & survival compare
    sns.pointplot(x="Pclass", y="Survived", hue="Sex", data=data_raw,
                  palette={"male": "blue", "female": "pink"},
                  markers=["*", "o"], linestyles=["-", "--"], ax = maxis2)

    #how does embark port factor with class, sex, and survival compare
    e = sns.FacetGrid(data_raw, col = 'Embarked')
    e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')
    e.add_legend()

    #plot distributions of age of passengers who survived or did not survive
    a = sns.FacetGrid( data_raw, hue = 'Survived', aspect=4 )
    a.map(sns.kdeplot, 'Age', shade= True )
    a.set(xlim=(0 , data_raw['Age'].max()))
    a.add_legend()

    #histogram comparison of sex, class, and age by survival
    h = sns.FacetGrid(data_raw, row = 'Sex', col = 'Pclass', hue = 'Survived')
    h.map(plt.hist, 'Age', alpha = .75)
    h.add_legend()

#    pair plots of entire dataset
# =============================================================================
#     pp = sns.pairplot(data_raw, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )
#     pp.set(xticklabels=[])
#     correlation_heatmap(data_raw)
# =============================================================================

def model():
    MLA = [
    #Ensemble Methods
    ensemble.AdaBoostClassifier(),
    ensemble.BaggingClassifier(),
    ensemble.ExtraTreesClassifier(),
    ensemble.GradientBoostingClassifier(),
    ensemble.RandomForestClassifier(),

    #Gaussian Processes
    gaussian_process.GaussianProcessClassifier(),

    #GLM
    linear_model.LogisticRegressionCV(),
    linear_model.PassiveAggressiveClassifier(),
    linear_model.RidgeClassifierCV(),
    linear_model.SGDClassifier(),
    linear_model.Perceptron(),

    #Navies Bayes
    naive_bayes.BernoulliNB(),
    naive_bayes.GaussianNB(),

    #Nearest Neighbor
    neighbors.KNeighborsClassifier(),

    #SVM
    svm.SVC(probability=True),
    svm.NuSVC(probability=True),
    svm.LinearSVC(),

    #Trees
    tree.DecisionTreeClassifier(),
    tree.ExtraTreeClassifier(),

    #Discriminant Analysis
    discriminant_analysis.LinearDiscriminantAnalysis(),
    discriminant_analysis.QuadraticDiscriminantAnalysis(),


    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html
    XGBClassifier()
    ]



    #split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit
    #note: this is an alternative to train_test_split
    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

    #index through MLA and save performance to table
    row_index = 0
    for alg in MLA:

        #set name and parameters
        MLA_name = alg.__class__.__name__
        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name
        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())

        #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate
        cv_results = model_selection.cross_validate(alg, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)

        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()
        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()
        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()
        #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets
        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!


        #save MLA predictions - see section 6 for usage
        alg.fit(data_raw[data_raw_x_bin], data_raw[Target])
        MLA_predict[MLA_name] = alg.predict(data_raw[data_raw_x_bin])

        row_index+=1


    #print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html
    MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)
    MLA_compare

    #.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')
    #prettify using pyplot: https://matplotlib.org/api/pyplot_api.html
    #plt.title('Machine Learning Algorithm Accuracy Score \n')
    #plt.xlabel('Accuracy Score (%)')
    #plt.ylabel('Algorithm')



def mytree(df):

    #initialize table to store predictions
    Model = pd.DataFrame(data = {'Predict':[]})
    male_title = ['Master'] #survived titles

    for index, row in df.iterrows():

        #Question 1: Were you on the Titanic; majority died
        Model.loc[index, 'Predict'] = 0

        #Question 2: Are you female; majority survived
        if (df.loc[index, 'Sex'] == 'female'):
                  Model.loc[index, 'Predict'] = 1

        #Question 3A Female - Class and Question 4 Embarked gain minimum information

        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0
        if ((df.loc[index, 'Sex'] == 'female') &
            (df.loc[index, 'Pclass'] == 3) &
            (df.loc[index, 'Embarked'] == 'S')  &
            (df.loc[index, 'Fare'] > 8)

           ):
                  Model.loc[index, 'Predict'] = 0

        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived
        if ((df.loc[index, 'Sex'] == 'male') &
            (df.loc[index, 'Title'] in male_title)
            ):
            Model.loc[index, 'Predict'] = 1


    return Model


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')


def evaluate():
    pivot_female = data_raw[data_raw.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()
    print('Survival Decision Tree w/Female Node: \n',pivot_female)

    pivot_male = data_raw[data_raw.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()
    print('\n\nSurvival Decision Tree w/Male Node: \n',pivot_male)

    Tree_Predict = mytree(data_raw)
    print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\n'.format(metrics.accuracy_score(data_raw['Survived'], Tree_Predict)*100))

    print(metrics.classification_report(data_raw['Survived'], Tree_Predict))

    # Compute confusion matrix
    cnf_matrix = metrics.confusion_matrix(data_raw['Survived'], Tree_Predict)
    np.set_printoptions(precision=2)

    class_names = ['Dead', 'Survived']
    # Plot non-normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names,
                          title='Confusion matrix, without normalization')

    # Plot normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,
                          title='Normalized confusion matrix')

def tuneModel():

# =============================================================================
#     Tune Model with Hyper-Parameters
# =============================================================================
    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

    #base model
    dtree = tree.DecisionTreeClassifier(random_state = 0)
    base_results = model_selection.cross_validate(dtree, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)
    dtree.fit(data_raw[data_raw_x_bin], data_raw[Target])

    print('BEFORE DT Parameters: ', dtree.get_params())
    print("BEFORE DT Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100))
    print("BEFORE DT Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
    print("BEFORE DT Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
    #print("BEFORE DT Test w/bin set score min: {:.2f}". format(base_results['test_score'].min()*100))
    print('-'*10)


    #tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier
    param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini
                  #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best
                  'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none
                  #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2
                  #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1
                  #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all
                  'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation
                 }

    #print(list(model_selection.ParameterGrid(param_grid)))

    #choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search
    #http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html
    tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)
    tune_model.fit(data_raw[data_raw_x_bin], data_raw[Target])

    #print(tune_model.cv_results_.keys())
    #print(tune_model.cv_results_['params'])
    print('AFTER DT Parameters: ', tune_model.best_params_)
    #print(tune_model.cv_results_['mean_train_score'])
    print("AFTER DT Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))
    #print(tune_model.cv_results_['mean_test_score'])
    print("AFTER DT Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
    print("AFTER DT Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
    print('-'*10)


    #duplicates gridsearchcv
    #tune_results = model_selection.cross_validate(tune_model, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)

    #print('AFTER DT Parameters: ', tune_model.best_params_)
    #print("AFTER DT Training w/bin set score mean: {:.2f}". format(tune_results['train_score'].mean()*100))
    #print("AFTER DT Test w/bin set score mean: {:.2f}". format(tune_results['test_score'].mean()*100))
    #print("AFTER DT Test w/bin set score min: {:.2f}". format(tune_results['test_score'].min()*100))
    #print('-'*10)


# =============================================================================
#     Tune Model with Feature Selection
# =============================================================================

    #base model
    print('BEFORE DT RFE Training Shape Old: ', data_raw[data_raw_x_bin].shape)
    print('BEFORE DT RFE Training Columns Old: ', data_raw[data_raw_x_bin].columns.values)

    print("BEFORE DT RFE Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100))
    print("BEFORE DT RFE Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
    print("BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
    print('-'*10)



    #feature selection
    dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)
    dtree_rfe.fit(data_raw[data_raw_x_bin], data_raw[Target])

    #transform x&y to reduced features and fit new model
    #alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
    X_rfe = data_raw[data_raw_x_bin].columns.values[dtree_rfe.get_support()]
    rfe_results = model_selection.cross_validate(dtree, data_raw[X_rfe], data_raw[Target], cv  = cv_split)

    #print(dtree_rfe.grid_scores_)
    print('AFTER DT RFE Training Shape New: ', data_raw[X_rfe].shape)
    print('AFTER DT RFE Training Columns New: ', X_rfe)

    print("AFTER DT RFE Training w/bin score mean: {:.2f}". format(rfe_results['train_score'].mean()*100))
    print("AFTER DT RFE Test w/bin score mean: {:.2f}". format(rfe_results['test_score'].mean()*100))
    print("AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}". format(rfe_results['test_score'].std()*100*3))
    print('-'*10)


    #tune rfe model
    rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)
    rfe_tune_model.fit(data_raw[X_rfe], data_raw[Target])

    #print(rfe_tune_model.cv_results_.keys())
    #print(rfe_tune_model.cv_results_['params'])
    print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)
    #print(rfe_tune_model.cv_results_['mean_train_score'])
    print("AFTER DT RFE Tuned Training w/bin score mean: {:.2f}". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))
    #print(rfe_tune_model.cv_results_['mean_test_score'])
    print("AFTER DT RFE Tuned Test w/bin score mean: {:.2f}". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
    print("AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
    print('-'*10)

    dot_data = tree.export_graphviz(dtree, out_file=None,
                                feature_names = data_raw_x_bin, class_names = True,
                                filled = True, rounded = True)
    graph = graphviz.Source(dot_data)
    graph


def validate():
    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%
    vote_est = [
        #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html
        ('ada', ensemble.AdaBoostClassifier()),
        ('bc', ensemble.BaggingClassifier()),
        ('etc',ensemble.ExtraTreesClassifier()),
        ('gbc', ensemble.GradientBoostingClassifier()),
        ('rfc', ensemble.RandomForestClassifier()),

        #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc
        ('gpc', gaussian_process.GaussianProcessClassifier()),

        #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
        ('lr', linear_model.LogisticRegressionCV()),

        #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html
        ('bnb', naive_bayes.BernoulliNB()),
        ('gnb', naive_bayes.GaussianNB()),

        #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html
        ('knn', neighbors.KNeighborsClassifier()),

        #SVM: http://scikit-learn.org/stable/modules/svm.html
        ('svc', svm.SVC(probability=True)),

        #xgboost: http://xgboost.readthedocs.io/en/latest/model.html
       ('xgb', XGBClassifier())

    ]


    #Hard Vote or majority rules
    vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')
    vote_hard_cv = model_selection.cross_validate(vote_hard, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)
    vote_hard.fit(data_raw[data_raw_x_bin], data_raw[Target])

    print("Hard Voting Training w/bin score mean: {:.2f}". format(vote_hard_cv['train_score'].mean()*100))
    print("Hard Voting Test w/bin score mean: {:.2f}". format(vote_hard_cv['test_score'].mean()*100))
    print("Hard Voting Test w/bin score 3*std: +/- {:.2f}". format(vote_hard_cv['test_score'].std()*100*3))
    print('-'*10)


    #Soft Vote or weighted probabilities
    vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')
    vote_soft_cv = model_selection.cross_validate(vote_soft, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)
    vote_soft.fit(data_raw[data_raw_x_bin], data_raw[Target])

    print("Soft Voting Training w/bin score mean: {:.2f}". format(vote_soft_cv['train_score'].mean()*100))
    print("Soft Voting Test w/bin score mean: {:.2f}". format(vote_soft_cv['test_score'].mean()*100))
    print("Soft Voting Test w/bin score 3*std: +/- {:.2f}". format(vote_soft_cv['test_score'].std()*100*3))
    print('-'*10)

    #Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
    grid_n_estimator = [10, 50, 100, 300]
    grid_ratio = [.1, .25, .5, .75, 1.0]
    grid_learn = [.01, .03, .05, .1, .25]
    grid_max_depth = [2, 4, 6, 8, 10, None]
    grid_min_samples = [5, 10, .03, .05, .10]
    grid_criterion = ['gini', 'entropy']
    grid_bool = [True, False]
    grid_seed = [0]


    grid_param = [
                [{
                #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
                'n_estimators': grid_n_estimator, #default=50
                'learning_rate': grid_learn, #default=1
                #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R
                'random_state': grid_seed
                }],


                [{
                #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier
                'n_estimators': grid_n_estimator, #default=10
                'max_samples': grid_ratio, #default=1.0
                'random_state': grid_seed
                 }],


                [{
                #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier
                'n_estimators': grid_n_estimator, #default=10
                'criterion': grid_criterion, #default=”gini”
                'max_depth': grid_max_depth, #default=None
                'random_state': grid_seed
                 }],


                [{
                #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier
                #'loss': ['deviance', 'exponential'], #default=’deviance’
                'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.
                'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.
                #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”
                'max_depth': grid_max_depth, #default=3
                'random_state': grid_seed
                 }],


                [{
                #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier
                'n_estimators': grid_n_estimator, #default=10
                'criterion': grid_criterion, #default=”gini”
                'max_depth': grid_max_depth, #default=None
                'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.
                'random_state': grid_seed
                 }],

                [{
                #GaussianProcessClassifier
                'max_iter_predict': grid_n_estimator, #default: 100
                'random_state': grid_seed
                }],


                [{
                #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV
                'fit_intercept': grid_bool, #default: True
                #'penalty': ['l1','l2'],
                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs
                'random_state': grid_seed
                 }],


                [{
                #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB
                'alpha': grid_ratio, #default: 1.0
                 }],


                #GaussianNB -
                [{}],

                [{
                #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier
                'n_neighbors': [1,2,3,4,5,6,7], #default: 5
                'weights': ['uniform', 'distance'], #default = ‘uniform’
                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
                }],


                [{
                #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC
                #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r
                #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                'C': [1,2,3,4,5], #default=1.0
                'gamma': grid_ratio, #edfault: auto
                'decision_function_shape': ['ovo', 'ovr'], #default:ovr
                'probability': [True],
                'random_state': grid_seed
                 }],


                [{
                #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html
                'learning_rate': grid_learn, #default: .3
                'max_depth': [1,2,4,6,8,10], #default 2
                'n_estimators': grid_n_estimator,
                'seed': grid_seed
                 }]
            ]



    start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter
    for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip

        #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm
        #print(param)


        start = time.perf_counter()
        best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')
        best_search.fit(data_raw[data_raw_x_bin], data_raw[Target])
        run = time.perf_counter() - start

        best_param = best_search.best_params_
        print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))
        clf[1].set_params(**best_param)


    run_total = time.perf_counter() - start_total
    print('Total optimization time was {:.2f} minutes.'.format(run_total/60))

    print('-'*10)

    grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')
    grid_hard_cv = model_selection.cross_validate(grid_hard, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)
    grid_hard.fit(data_raw[data_raw_x_bin], data_raw[Target])

    print("Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}". format(grid_hard_cv['train_score'].mean()*100))
    print("Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}". format(grid_hard_cv['test_score'].mean()*100))
    print("Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}". format(grid_hard_cv['test_score'].std()*100*3))
    print('-'*10)

    #Soft Vote or weighted probabilities w/Tuned Hyperparameters
    grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')
    grid_soft_cv = model_selection.cross_validate(grid_soft, data_raw[data_raw_x_bin], data_raw[Target], cv  = cv_split)
    grid_soft.fit(data_raw[data_raw_x_bin], data_raw[Target])

    print("Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}". format(grid_soft_cv['train_score'].mean()*100))
    print("Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}". format(grid_soft_cv['test_score'].mean()*100))
    print("Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}". format(grid_soft_cv['test_score'].std()*100*3))
    print('-'*10)

    print(testDF.info())
    print("-"*10)

    testDF['Survived'] = mytree(testDF).astype(int)
    testDF['Survived'] = grid_hard.predict(testDF[data_raw_x_bin])
    submit = testDF[['PassengerId','Survived']]
    submit.to_csv("../working/submit.csv", index=False)

    print('Validation Data Distribution: \n', testDF['Survived'].value_counts(normalize = True))
    submit.sample(10)



if __name__ == "__main__":
    trainDF = pd.read_csv('train.csv')
    testDF = pd.read_csv('test.csv')

    data_raw = trainDF.copy(deep=True)
    data_clean = [data_raw, testDF]
    data_clean = cleanData(data_clean)


    """
    Split Training and Testing Data

    """
    #define y variable aka target/outcome
    Target = ['Survived']

    #define x variables for original features aka feature selection
    data_raw_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts
    data_raw_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation
    data_raw_xy =  Target + data_raw_x
    print('Original X Y: ', data_raw_xy, '\n')


    #define x variables for original w/bin features to remove continuous variables
    data_raw_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']
    data_raw_xy_bin = Target + data_raw_x_bin
    print('Bin X Y: ', data_raw_xy_bin, '\n')


    #define x and y variables for dummy features original
    data_raw_dummy = pd.get_dummies(data_raw[data_raw_x])
    data_raw_x_dummy = data_raw_dummy.columns.tolist()
    data_raw_xy_dummy = Target + data_raw_x_dummy
    print('Dummy X Y: ', data_raw_xy_dummy, '\n')

    #split train and test data with function defaults
    #random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation
    train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data_raw[data_raw_x_calc], data_raw[Target], random_state = 0)
    train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data_raw[data_raw_x_bin], data_raw[Target] , random_state = 0)
    train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data_raw_dummy[data_raw_x_dummy], data_raw[Target], random_state = 0)


    print("data_raw Shape: {}".format(data_raw.shape))
    print("Train1 Shape: {}".format(train1_x.shape))
    print("Test1 Shape: {}".format(test1_x.shape))

    train1_x_bin.head()

    analysis()

    #create table to compare MLA metrics
    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']
    MLA_compare = pd.DataFrame(columns = MLA_columns)

    #create table to compare MLA predictions
    MLA_predict = data_raw[Target]
    model()

    evaluate()

    tuneModel()

    validate()


    #print("Train Data:\n" ,data_raw.isnull().sum())
    #print("Test Data:\n", testDF.isnull().sum())


